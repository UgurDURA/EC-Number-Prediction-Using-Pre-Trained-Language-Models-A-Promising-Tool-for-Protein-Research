\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{graphicx}
\usepackage{subcaption}


\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\captionsetup{
  font={footnotesize} % Set the font size for all captions to 8pt (footnotesize)
}




\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{EC Number Prediction Using Pre-Trained Language Models: A Promising Tool for Protein Research}

\author{\IEEEauthorblockN{Ugur Dura\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\textit{Informatics Department} \\
\textit{Technical University of Munich}\\
Munich, Germany \\
ugur.dura@tum.de}
}

\maketitle

\begin{abstract}
Enzyme Commission (EC) numbers play a vital role in accurately understanding enzyme functions and their impact on cellular metabolism. In this paper, we explore the potential of utilizing pre-trained language models for EC number prediction, with a specific focus on protein sequences as a rich source of information. Our objective is to leverage deep learning models, particularly the transformer BERT model, to uncover patterns corresponding to the topological sites of each enzyme that may lead to predict their functionalities. Through comprehensive experimentation, we demonstrate the successful utilization of the transformer BERT model for identifying and classifying de novo enzyme functions based on protein sequences retrieved from UniProt. By treating proteins as linguistic entities, we harness the power of language models to enhance EC number prediction with a high level of accuracy. While our results showcase the effectiveness of this approach, further research is required to improve the classification accuracy for all four digits of the EC number. Future work will focus on refining the training process and incorporating additional contextual information to further enhance the model's performance. In conclusion, our study presents a promising tool for EC number prediction using pre-trained language models. By bridging the gap between protein sequences and language processing, this approach opens up new possibilities for advancements in protein research, including drug discovery, protein engineering, and biomedical applications.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\vspace{3cm}

\section{Introduction}


Enzymes, a vital class of proteins within the human body, play a crucial role in catalyzing reactions and regulating various biological processes. Understanding their functions is essential for numerous applications, such as metagenomics, industrial biotechnology, and diagnosing enzyme-related diseases. However, traditional experimental techniques like enzymatic assays have become increasingly time-consuming and challenging to keep up with the rapidly growing number of newly discovered enzymes.

In recent years, computational methods have emerged as invaluable tools for predicting enzyme function and guiding experimental validation. The widely used Enzyme Commission (EC) system employs four-digit codes to specify enzyme functions, enabling computational methods to accurately annotate enzyme activities.

To address this issue, computational methods have emerged as valuable tools for predicting enzyme function and guiding experimental validation. Previous research has explored three main approaches for enzyme EC number prediction: (i) structure-based prediction, which involves first predicting enzyme structures and then assigning EC numbers based on similarities to known templates; (ii) sequence similarity-based methods, which predict enzyme function based on the assumption that enzymes with high sequence similarity tend to have similar functionalities; and (iii) machine learning-based methods, which extract features from enzyme sequences and use machine learning algorithms for classification.

Despite progress made, existing approaches encounter challenges such as homology requirements, feature design limitations, and feature dimensionality non-uniformity. Homology requirements can limit the prediction capability of sequence similarity-based methods when encountering sequences with limited homologies in existing databases. Moreover, feature design often relies on manually crafted or pre-defined features, which may not be optimal or sustainable in the omic era, with the rapid expansion of known enzyme sequences.

In contrast, recent advances in deep neural networks have shown promise as generative and discriminative models for protein science and engineering. Transformer-based language models, particularly, have demonstrated impressive capabilities in various domains, including natural language processing. To address these challenges, this research proposes a novel approach based on deep learning, specifically utilizing Prot-BERT and transfer learning, for predicting first-order EC numbers of enzymes.

In conclusion, this research aims to develop a robust and efficient deep learning-based predictive model for enzyme EC number classification. By combining the power of Prot-BERT with fine-tuning on a large dataset of enzyme sequences, this approach holds the potential to significantly advance our understanding of enzyme functions and aid in various applications related to enzymology.

\newpage


\section{Materials and Methods}\label{sec11}

\subsection{Data Collection}

The dataset used in this study was obtained from SwissProt, a manually curated section of UniProtKB, which contains a vast collection of annotated protein entries. Initially, the dataset comprised 565,254 protein sequences. To focus specifically on enzymes and their respective functions, a rigorous filtering process was applied, resulting in a refined dataset consisting of 271,464 entries. Only entries with at least one assigned EC number were retained for further analysis. In cases where an entry contained multiple EC numbers, the first one listed was used for subsequent prediction tasks.



\subsection{Data Analysis and Preprocessing}



To prepare our dataset for the deep learning model, we considered amino acids as the fundamental building blocks, much like an alphabet, used to train the Masked Language Model (MLM). Each amino acid was denoted by a shorthand one-letter symbol, such as 'M' for methionine, 'R' for arginine, and 'X' for unknown amino acids. This concise representation enabled us to effectively handle the diverse range of amino acid sequences present in the dataset. The distribution of amino acid letters in the dataset can be seen in Fig. \ref{fig:dataset_analysis}. Notably, leucine emerged as the most common amino acid, and our dataset did not contain any unknown amino acids.



\begin{figure}[htp!]
  \centering
  \includegraphics[width=0.8\linewidth]{images/UniqueAminoAcids.png}
  \caption{The plot illustrates the amino acid letter density distribution within the collected dataset. The y-axis represents the number of occurrences, while the x-axis displays the corresponding amino acid letters.}
  \label{fig:dataset_analysis}
\end{figure}





\begin{figure}[htp!]
  \centering
  \begin{subfigure}[b]{1\linewidth}
      \centering
    \includegraphics[width=1\linewidth]{images/Sequence_SizeDistribution(2000).png}
    \caption{}
    \label{fig:subfig1}
  \end{subfigure}
  \vspace{0.5cm} % Adjust the vertical space between subfigures
  \begin{subfigure}[b]{1\linewidth}
      \centering
    \includegraphics[width=1\linewidth]{images/Sequence_SizeDistribution(1000).png}
    \caption{}
    \label{fig:subfig2}
  \end{subfigure}
  \caption{The sequence length histogram of the dataset. The X-axis shows the sequence length and the Y-axis shows the number of sequences accumulated within that range. Left: Sequence length distribution histogram on the whole dataset. Right: Sequence length distribution histogram with range 0-1000.}
  \label{fig:Sequence_SizeDistribution}
\end{figure}





During the data analysis process, one crucial aspect that needed consideration was the variation in sequence lengths. To tokenize and train our model effectively, we needed to decide on a fixed array size for the sequences. Our analysis revealed a significant distribution of sequence lengths, with the majority falling within the range of 200 to 500 amino acids as shown in Fig. \ref*{fig:Sequence_SizeDistribution}. This finding presented a challenge, as we had to strike a balance between capturing relevant information and computational efficiency(Fig. \ref*{fig:Padding_Truncation}). Ultimately, an array size of 512 was chosen to accommodate the majority of sequences within this length range.



\begin{figure}[htp!]
  \centering
  \begin{subfigure}[b]{0.8\linewidth}
      \centering
    \includegraphics[width=0.8\linewidth]{images/XMASK_Padding_Truncation_Distribution.jpg}
    \caption{}
    \label{fig:subfig1}
  \end{subfigure}
  \vspace{0.5cm} % Adjust the vertical space between subfigures
  \begin{subfigure}[b]{0.8\linewidth}
      \centering
    \includegraphics[width=0.8\linewidth]{images/XMASK_Padding_Truncation_Distribution_1024.jpg}
    \caption{}
    \label{fig:subfig2}
  \end{subfigure}
  \caption{Representation of the padding and truncation rate of the sequence during tokenization. The X-axis represents the length of the sequence ( for (a) max length=512, for (b) max length= 1024), the Y-axis represents the data-set index(262K entry). Yellow shows the tokenized sequence and purple shows the padded area.}
  \label{fig:Padding_Truncation}
\end{figure}



At the end of this data preprocessing, we employed one-hot encoding to effectively serve the amino acid sequences into our deep learning model. One-hot encoding is a technique that converts categorical data, such as the shorthand one-letter symbols representing amino acids, into binary vectors. Each amino acid symbol was transformed into a binary vector of zeros and ones, with a value of one at the position corresponding to the specific amino acid and zeros elsewhere. Let A be a set containing specific amino acid letters, and $\mathbb{R}_A(x)$ be the indicator function. For each amino acid x, $\mathbb{R}_A(x)$ returns 1 if x is in the set A, and 0 if x is not in the set A (eq. \ref*{eq:indicator_function}).This encoding allowed the model to process the amino acid sequences as numerical inputs, facilitating the training and prediction processes.


\begin{subequations}
  \label{eq:indicator_function}
  \begin{equation}
  A = \left \{ a_1, a_2, a_3, \ldots, 21 \right \}
  \end{equation}
  
  \begin{equation}
  \mathbb{R}_A(x) =
  \begin{cases}
  1 & \text{if } x \in A, \\
  0 & \text{if } x \notin A.
  \end{cases}
  \end{equation}
  \end{subequations}



By utilizing one-hot encoding, we successfully converted the diverse range of amino acid representations in the dataset into a format suitable for input to the Masked Language Model. By understanding the characteristics of our dataset,  we were able to lay a strong foundation for the subsequent development and fine-tuning of our deep learning model. 



\subsection{Model Construction}


In our research, we utilized the main Bert Layer from the pre-trained ProtBERT-BFD
model as the embedding layer. The ”training” parameter of the layer was set to False
during fine-tuning to maintain the integrity of the pre-trained embedding mechanism. We
employed the mandatory ProtBERT-BFD tokenizer to tokenize our dataset. Following
the embedding layer, we used pooling, normalization, two dense layers (with sizes 128
and 32), a dropout layer, and a final dense layer with softmax activation function. The
tokenizer outputs, including the tokenized sequence and mask, were fed into the BERT
layer. The model’s output went through subsequent layers until the final output layer,
which provided probability scores for each class. ReLU activation function was used for
hidden layers to prevent the vanishing gradient problem and enhance model performance.


\begin{figure}[htp!]
  \centering
  \includegraphics[width=1\linewidth]{images/Model_0001.jpg}
  \caption{Enzyme Commission number prediction up to two digits model.Hyperparameters for this model: Dataset size:
  50.000, Batch size: 32, Input array size: 512, Label array size: 72, epoch number: 10, optimizer:
  Adam, learning rate: 0.01, training and test split size: 1:9, activation function: sigmoid, output layer
  activation function: sigmoid.}
  \label{fig:model}
\end{figure}





\section{Results}

\subsection{Results of first-order EC number prediction}

Upon completing the training of our model, we conducted an assessment of various
metrics including loss, accuracy, validation accuracy, and validation loss. fig. 5 demon-
strates a continuous decrease in loss and an increase in accuracy, indicating that our
model did not suffer from overfitting and the training process was successful.

Additionally, following the model’s successful training, we performed several tests by
loading data and predicting sequences provided by the user. The outcomes of these tests
confirmed that our model possesses the capability to accurately predict EC numbers for
the requested sequences.

\subsection{Results of second, third and fourth-order EC number prediction}

We extended our predictions to include second, third, and fourth orders of the EC number
to obtain more specific information about the enzymes. As the order of the EC number
increased, the number of classes (labels) also increased. For example, the second order
had 72 classes, the third order had 207 classes, and the fourth order had 272 distinct
classes. Due to the increasing complexity of the EC numbers, predicting the exact EC
number based on a limited dataset became more challenging. However, we made every
effort to achieve the best possible results, as shown in fig. 5.


\begin{figure}[ht]
  \centering

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=\linewidth]{plot1}
    \caption{Subcaption 1}
    \label{fig:subfig1}
  \end{subfigure}

  \vspace{0.5cm}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=\linewidth]{plot2}
    \caption{Subcaption 2}
    \label{fig:subfig2}
  \end{subfigure}

  \vspace{0.5cm}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=\linewidth]{plot3}
    \caption{Subcaption 3}
    \label{fig:subfig3}
  \end{subfigure}

  \vspace{0.5cm}

  \begin{subfigure}{0.45\linewidth}
    \includegraphics[width=\linewidth]{plot4}
    \caption{Subcaption 4}
    \label{fig:subfig4}
  \end{subfigure}

  \caption{Main caption for the entire figure}
  \label{fig:mainfigure}
\end{figure}










\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}



\appendix

\section*{Plots}


sdfsdfsdf

\section*{Tables}


sdfsdf






\end{document}
